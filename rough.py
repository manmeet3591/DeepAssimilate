# -*- coding: utf-8 -*-
"""data_assimilation_package.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tyvUidk7xx5g37nVTOyKg6hRL-vaqBJI
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import cv2

# SRCNN model
class SRCNN(nn.Module):
    def __init__(self):
        super(SRCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 64, kernel_size=9, padding=4)
        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)
        self.conv3 = nn.Conv2d(32, 1, kernel_size=5, padding=2)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.conv3(x)
        return x

# Function to generate random noise images
def generate_random_image(seed, img_size=(256, 256)):
    np.random.seed(seed)
    random_image = np.random.rand(*img_size)
    return random_image

# Bicubic interpolation for downscaling
def bicubic_downscale(image, scale_factor):
    height, width = image.shape
    new_size = (int(width // scale_factor), int(height // scale_factor))
    downscaled_image = cv2.resize(image, new_size, interpolation=cv2.INTER_CUBIC)
    return downscaled_image

# Bicubic upscaling to original size
def bicubic_upscale(image, original_size):
    upscaled_image = cv2.resize(image, original_size, interpolation=cv2.INTER_CUBIC)
    return upscaled_image

# Function to create "stations" image with 90% data missing
def create_stations_image(original_image, gap_ratio=0.9):
    mask = np.random.rand(*original_image.shape) < gap_ratio
    stations_image = original_image.copy()
    stations_image[mask] = np.nan  # Mask out 90% of the data
    return stations_image, mask  # Also return the mask for loss calculation

# Example usage
seed = 42
img_size = (256, 256)

# Generate random input image
original_image = generate_random_image(seed, img_size)

original_image.shape

# Input Image: 4x downscaled and 2x upscaled
input_image_4x = bicubic_downscale(original_image, 4)
input_image_4x_upscaled_2x = bicubic_upscale(input_image_4x, (int(original_image.shape[0]/2), int(original_image.shape[1]/2)))

# Target Image: 2x downscaled
target_image_2x = bicubic_downscale(original_image, 2)

print(input_image_4x_upscaled_2x.shape, target_image_2x.shape)

# Create "stations" image (ground truth with 90% missing data)
stations_image, mask = create_stations_image(original_image, gap_ratio=0.9)
# stations_image.shape

# Nearest neighbor interpolation for resizing stations_image
def nearest_neighbor_resize(image, target_size):
    resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_NEAREST)
    return resized_image

# Resize stations_image to the shape of target_image_2x using nearest neighbor interpolation
stations_image_resized = nearest_neighbor_resize(stations_image, target_image_2x.shape)

print("Resized stations image shape:", stations_image_resized.shape)

"""# Give a figure on how nearest neighbour interpolation was handled"""

import torch
import numpy as np
import cv2
from sklearn.model_selection import train_test_split

# Function to generate a random image with multiple channels (e.g., 3 for RGB)
def generate_random_image(seed, img_size, num_channels):
    np.random.seed(seed)
    # Generate random image with shape (C, H, W), where C is the number of channels
    return np.random.rand(num_channels, img_size[0], img_size[1])

# Function to downscale an image (multi-channel supported)
def bicubic_downscale(image, scale_factor):
    # Resize each channel using bicubic interpolation
    channels = [cv2.resize(image[c], (image.shape[2] // scale_factor, image.shape[1] // scale_factor), interpolation=cv2.INTER_CUBIC) for c in range(image.shape[0])]
    return np.stack(channels, axis=0)

# Function to upscale an image (multi-channel supported)
def bicubic_upscale(image, target_size):
    # Resize each channel using bicubic interpolation
    channels = [cv2.resize(image[c], target_size, interpolation=cv2.INTER_CUBIC) for c in range(image.shape[0])]
    return np.stack(channels, axis=0)

# Function to create the stations image with missing data (set missing values to NaN)
def create_stations_image(image, gap_ratio):
    mask = np.random.rand(*image.shape) > gap_ratio
    stations_image = np.where(mask, image, np.nan)  # Set values to NaN where mask is False
    return stations_image, mask

# # Nearest neighbor interpolation for resizing stations_image (multi-channel supported)
# def nearest_neighbor_resize(image, target_size):
#     channels = [cv2.resize(image[c], target_size, interpolation=cv2.INTER_NEAREST) for c in range(image.shape[0])]
#     return np.stack(channels, axis=0)

import numpy as np
import cv2

# Function to resize images while preserving NaN values

# Give a figure on how nearest neighbour interpolation was handled

def nearest_neighbor_resize_with_nan(image, target_size):
    # Create a mask for NaNs
    nan_mask = np.isnan(image)

    # Replace NaNs with a placeholder value (e.g., 0) before resizing
    image_filled = np.where(nan_mask, 0, image)

    # Perform nearest neighbor resizing
    channels = [cv2.resize(image_filled[c], target_size, interpolation=cv2.INTER_NEAREST) for c in range(image_filled.shape[0])]

    # Resize the mask separately
    resized_nan_mask = [cv2.resize(nan_mask[c].astype(np.uint8), target_size, interpolation=cv2.INTER_NEAREST) for c in range(nan_mask.shape[0])]

    # Stack channels and mask
    resized_image = np.stack(channels, axis=0)
    resized_nan_mask = np.stack(resized_nan_mask, axis=0).astype(bool)

    # Restore NaN values in the resized image
    resized_image_with_nan = np.where(resized_nan_mask, np.nan, resized_image)

    return resized_image_with_nan

# Generate random input image with multiple channels
seed = 42
img_size = (256, 256)
num_channels = 100  # For RGB or more channels if needed
original_image = generate_random_image(seed, img_size, num_channels)

# Input Image: 4x downscaled and 2x upscaled
input_image_4x = bicubic_downscale(original_image, 4)
input_image_4x_upscaled_2x = bicubic_upscale(input_image_4x, (original_image.shape[2] // 2, original_image.shape[1] // 2))

# Target Image: 2x downscaled
target_image_2x = bicubic_downscale(original_image, 2)

print("Input image (4x upscaled to 2x) shape:", input_image_4x_upscaled_2x.shape)
print("Target image (2x downscaled) shape:", target_image_2x.shape)



# Create "stations" image (ground truth with 90% missing data)
stations_image, mask = create_stations_image(original_image, gap_ratio=0.99)
# Apply the new resizing function
stations_image_resized = nearest_neighbor_resize_with_nan(stations_image, (target_image_2x.shape[1], target_image_2x.shape[2]))
# stations_image_resized = nearest_neighbor_resize(stations_image, (target_image_2x.shape[1], target_image_2x.shape[2]))

print("Resized stations image shape:", stations_image_resized.shape)

# Convert the images to PyTorch tensors (multi-channel)
input_image_4x_upscaled_2x_tensor = torch.tensor(input_image_4x_upscaled_2x, dtype=torch.float32)
target_image_2x_tensor = torch.tensor(target_image_2x, dtype=torch.float32)
stations_image_resized_tensor = torch.tensor(stations_image_resized, dtype=torch.float32)

# Verify the shapes of the tensors
print(f"Input tensor shape: {input_image_4x_upscaled_2x_tensor.shape}")
print(f"Target tensor shape: {target_image_2x_tensor.shape}")
print(f"Stations tensor shape: {stations_image_resized_tensor.shape}")

print(np.isnan(stations_image).sum())

print(np.isnan(stations_image_resized).sum())

print(stations_image.shape)
print(stations_image_resized.shape)

# Fixing the issue by removing .numpy() for numpy arrays, keeping it only for PyTorch tensors
import matplotlib.pyplot as plt
# Updated function to handle both numpy arrays and PyTorch tensors
def visualize_channel(image_tensor, channel_index, title):
    # Check if it's a PyTorch tensor and convert it to a numpy array if necessary
    if isinstance(image_tensor, torch.Tensor):
        image_channel = image_tensor[channel_index].numpy()
    else:
        image_channel = image_tensor[channel_index]
    plt.imshow(image_channel, cmap='gray')
    plt.title(title)
    plt.axis('off')
    plt.show()

# Convert images to PyTorch tensors
input_image_4x_upscaled_2x_tensor = torch.tensor(input_image_4x_upscaled_2x, dtype=torch.float32)
target_image_2x_tensor = torch.tensor(target_image_2x, dtype=torch.float32)
stations_image_resized_tensor = torch.tensor(stations_image_resized, dtype=torch.float32)

# Visualize the first channel (e.g., Red channel) from each tensor
# visualize_channel(original_image, 0, 'Original Image - Channel 0')
# visualize_channel(input_image_4x_upscaled_2x_tensor, 0, 'Input Image 4x Upscaled to 2x - Channel 0')
# visualize_channel(target_image_2x_tensor, 0, 'Target Image 2x Downscaled - Channel 0')
# visualize_channel(stations_image_resized_tensor, 0, 'Stations Image Resized - Channel 0')

print(torch.isnan(stations_image_resized_tensor).sum())

import matplotlib.pyplot as plt
import torch

# Updated function to handle both numpy arrays and PyTorch tensors
def visualize_channel(ax, image_tensor, channel_index, title):
    # Check if it's a PyTorch tensor and convert it to a numpy array if necessary
    if isinstance(image_tensor, torch.Tensor):
        image_channel = image_tensor[channel_index].numpy()
    else:
        image_channel = image_tensor[channel_index]
    ax.imshow(image_channel, cmap='gray')
    ax.set_title(title)
    ax.axis('off')

# Convert images to PyTorch tensors
input_image_4x_upscaled_2x_tensor = torch.tensor(input_image_4x_upscaled_2x, dtype=torch.float32)
target_image_2x_tensor = torch.tensor(target_image_2x, dtype=torch.float32)
stations_image_resized_tensor = torch.tensor(stations_image_resized, dtype=torch.float32)

# Create subplots
fig, axs = plt.subplots(1, 4, figsize=(20, 5))  # 1 row, 4 columns for each image

# Visualize the first channel (e.g., Red channel) from each tensor
# visualize_channel(axs[0], original_image, 0, 'Original High Resolution Image')
# visualize_channel(axs[1], input_image_4x_upscaled_2x_tensor, 0, 'Input Image (Original Image coarsened 4x)')
# visualize_channel(axs[2], target_image_2x_tensor, 0, 'Target Image (Original Image coarsened 2x)')
# visualize_channel(axs[3], stations_image_resized_tensor, 0, 'Stations Image (retaining 1% Original Image)')

# Adjust layout for better spacing
# plt.tight_layout()
# plt.show()

# original_image.shape

# input_image_4x_upscaled_2x_tensor.shape

# target_image_2x_tensor.shape

# stations_image_resized_tensor.shape

# stations_image_resized_tensor[0,:,:]

print(torch.nansum(stations_image_resized_tensor[0,:,:]))

def torch_nanmax(tensor):
    # Replace NaN values with a very large negative number (-inf)
    tensor_no_nan = torch.where(torch.isnan(tensor), torch.tensor(float('-inf'), device=tensor.device), tensor)

    # Apply the max function
    return torch.max(tensor_no_nan)
# torch_nanmax(stations_image_resized_tensor)

# ~torch.isnan(stations_image_resized_tensor[0,:,:].float())

print(stations_image_resized_tensor.dtype)

test_tensor = torch.tensor([[float('nan'), 1.0], [0.5, float('nan')]])
print(test_tensor)
print(torch.isnan(test_tensor))

# import torch
# from torch import nn
# from torch.utils.data import DataLoader, Dataset
# from torchvision import transforms
# import torch.optim as optim
# from copy import deepcopy

# # Dataset definition remains the same
# class ncDataset(Dataset):
#     def __init__(self, data, targets, stations):
#         self.data = data
#         self.targets = targets
#         self.stations = stations

#     def __getitem__(self, index):
#         x = torch.from_numpy(self.data[index]).unsqueeze(0)
#         y = torch.from_numpy(self.targets[index]).unsqueeze(0)
#         z = torch.from_numpy(self.stations[index]).unsqueeze(0)
#         return x, y, z

#     def __len__(self):
#         return len(self.data)

import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
import torch.optim as optim
from copy import deepcopy

# Dataset definition remains the same
class ncDataset(Dataset):
    def __init__(self, data, targets, stations):
        # Assuming data, targets, and stations are already PyTorch tensors
        self.data = data
        self.targets = targets
        self.stations = stations

    def __getitem__(self, index):
        x = self.data[index].unsqueeze(0)  # No need for torch.from_numpy()
        y = self.targets[index].unsqueeze(0)  # Already a tensor
        z = self.stations[index].unsqueeze(0)  # Already a tensor
        return x, y, z

    def __len__(self):
        return len(self.data)

# SRCNN Model definition remains the same
class SRCNN(nn.Module):
    def __init__(self):
        super(SRCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 64, kernel_size=9, padding=4)
        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)
        self.conv3 = nn.Conv2d(32, 1, kernel_size=5, padding=2)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.conv3(x)
        return x

# def masked_mse_loss(output, target):
#     # Create a mask for non-NaN values in the target tensor
#     mask = ~torch.isnan(target)

#     # Apply the mask to both the output and target
#     masked_output = output[mask]
#     masked_target = target[mask]

#     # Compute MSE loss only on valid (non-NaN) elements
#     return nn.functional.mse_loss(masked_output, masked_target)

def masked_mse_loss(output, target):
    # Create a mask for non-NaN values in both the target and output tensors
    mask = ~torch.isnan(target) & ~torch.isnan(output)

    # Apply the mask to both the output and target
    masked_output = output[mask]
    masked_target = target[mask]
    # print('sum of target = ', torch.nansum(target))
    # Diagnostic: Print how many valid (non-NaN) elements remain
    # print(f'Valid elements for loss calculation: {masked_target.numel()}')
    # print(torch.nansum(target))

    # Check if the mask has selected any valid (non-NaN) elements
    if masked_output.numel() == 0:  # No valid elements to compute loss
        return torch.tensor(0.0, device=output.device)  # Return a zero loss if there are no valid elements

    # Compute MSE loss only on valid (non-NaN) elements
    return nn.functional.mse_loss(masked_output, masked_target)


# Modified train function to include the masked MSE loss for stations_image_resized_tensor
def train(model, train_dataloader, val_dataloader, criterion, optimizer, device):
    model.train()
    train_loss = 0.0
    for batch in train_dataloader:
        lr, hr, station = batch
        lr, hr, station = lr.to(device), hr.to(device), station.to(device)
        optimizer.zero_grad()

        sr = model(lr)

        # First loss: Comparing model output with the original high-resolution image
        loss1 = criterion(sr, hr)

        # Second loss: Using masked MSE loss to ignore NaNs in the stations_image_resized_tensor
        loss2 = masked_mse_loss(sr, station)
        # print('loss2 = ', loss2)
        # Combine the losses (you can adjust the weights as needed)
        total_loss = loss1 + loss2

        total_loss.backward()
        optimizer.step()
        train_loss += total_loss.item()

    train_loss /= len(train_dataloader)

    # Validation
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for batch in val_dataloader:
            lr, hr, station = batch
            lr, hr, station = lr.to(device), hr.to(device), station.to(device)

            sr = model(lr)

            # Validation losses
            val_loss1 = criterion(sr, hr)
            val_loss2 = masked_mse_loss(sr, station)

            total_val_loss = val_loss1 + val_loss2
            val_loss += total_val_loss.item()

    val_loss /= len(val_dataloader)

    return train_loss, val_loss

x_train_patches = input_image_4x_upscaled_2x_tensor
y_train_patches = target_image_2x_tensor
z_train_patches = stations_image_resized_tensor

x_train_max = x_train_patches.max()
y_train_max = y_train_patches.max()
z_train_max = torch_nanmax(z_train_patches)
x_train_patches /= x_train_max
y_train_patches /= y_train_max
z_train_patches /= z_train_max

x_val_patches = x_train_patches[60:80]
y_val_patches = y_train_patches[60:80]
z_val_patches = z_train_patches[60:80]

x_test_patches = x_train_patches[80:]
y_test_patches = y_train_patches[80:]
z_test_patches = z_train_patches[80:]

x_train_patches = x_train_patches[:60]
y_train_patches = y_train_patches[:60]
z_train_patches = z_train_patches[:60]

torch.nansum(stations_image_resized_tensor)

# Dataset Preparation
train_dataset = ncDataset(x_train_patches, y_train_patches, z_train_patches)
val_dataset = ncDataset(x_val_patches, y_val_patches, z_val_patches)
test_dataset = ncDataset(x_test_patches, y_test_patches, z_test_patches)
train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=20, shuffle=True)

from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter("runs/srcnn")

# Initialize the model, loss function, and optimizer
device = 'cuda'
model = SRCNN().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training and validation loop with early stopping
num_epochs = 1000
print_interval = 10
patience = 500
best_val_loss = float('inf')
counter = 0
best_model = None
is_train = True

if is_train:
    for epoch in range(1, num_epochs + 1):
        train_loss, val_loss = train(model, train_dataloader, val_dataloader, criterion, optimizer, device)

        # Log losses to TensorBoard
        writer.add_scalars("Loss", {"Train": train_loss, "Validation": val_loss}, epoch)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model = deepcopy(model)
            counter = 0
        else:
            counter += 1

        if epoch % print_interval == 0:
            print(f"Epoch [{epoch}/{num_epochs}] - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}")

        if counter >= patience:
            print("Early stopping triggered.")
            break

    writer.close()

# Save the best model
model_save_path = "best_model_srcnn.pth"
if is_train:
    torch.save(best_model.state_dict(), model_save_path)

# Load the best model
loaded_model = SRCNN().to(device)
loaded_model.load_state_dict(torch.load(model_save_path))
loaded_model.eval()

"""# Performance on test data"""

import torch

def evaluate(model, test_dataloader, criterion, device):
    model.eval()
    test_loss = 0.0
    with torch.no_grad():
        for batch in test_dataloader:
            lr, hr, station = batch
            lr, hr, station = lr.to(device), hr.to(device), station.to(device)

            sr = model(lr)

            # Test loss components
            test_loss1 = criterion(sr, hr)  # Compare to target high-resolution image
            test_loss2 = masked_mse_loss(sr, station)  # Masked MSE loss for stations image

            total_test_loss = test_loss1 + test_loss2
            test_loss += total_test_loss.item()

    test_loss /= len(test_dataloader)
    return test_loss

# Test dataset DataLoader
test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=False)

# Evaluate the model on the test dataset
test_loss = evaluate(loaded_model, test_dataloader, criterion, device)
print(f"Test Loss: {test_loss:.4f}")

from sklearn.metrics import r2_score

def calculate_r2(model, test_dataloader, device):
    model.eval()
    all_sr = []
    all_hr = []
    with torch.no_grad():
        for batch in test_dataloader:
            lr, hr, _ = batch  # Only use low-resolution (lr) and high-resolution target (hr)
            lr, hr = lr.to(device), hr.to(device)

            sr = model(lr)  # Predicted super-resolved image

            # Collect outputs and targets for R² calculation
            all_sr.append(sr.cpu().numpy())
            all_hr.append(hr.cpu().numpy())

    # Convert to NumPy arrays
    all_sr = np.concatenate(all_sr, axis=0).reshape(-1)
    all_hr = np.concatenate(all_hr, axis=0).reshape(-1)

    # Compute R²
    return r2_score(all_hr, all_sr)

# Calculate R² for the test dataset
r2 = calculate_r2(loaded_model, test_dataloader, device)
print(f"R² Score: {r2:.4f}")

"""# Diffusion"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from diffusers import DDPMScheduler, UNet2DModel
from copy import deepcopy

class ncDataset(Dataset):
    def __init__(self, data, targets, targets_z):
        self.data = data
        self.targets = targets
        self.targets_z = targets_z

    def __getitem__(self, index):
        # Convert to tensor if the data is not already a tensor
        x = self.data[index]
        if not isinstance(x, torch.Tensor):
            x = torch.from_numpy(x).unsqueeze(0)
        else:
            x = x.unsqueeze(0)

        y = self.targets[index]
        if not isinstance(y, torch.Tensor):
            y = torch.from_numpy(y).unsqueeze(0)
        else:
            y = y.unsqueeze(0)

        z = self.targets_z[index]
        if not isinstance(z, torch.Tensor):
            z = torch.from_numpy(z).unsqueeze(0)
        else:
            z = z.unsqueeze(0)

        return x, y, z

    def __len__(self):
        return len(self.data)

def train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, device):
    model.train()
    train_loss = 0.0
    train_loss_z = 0.0  # For additional loss

    for batch in train_dataloader:
        lr, hr, zr = batch  # Low resolution (lr), high resolution (hr), and z
        lr, hr, zr = lr.to(device), hr.to(device), zr.to(device)

        # Sample noise to add to the high-resolution images
        noise = torch.randn_like(hr)

        # Sample a random timestep for each image
        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (lr.shape[0],)).long().to(device)

        # Get the noisy high-resolution images
        noisy_images = scheduler.add_noise(hr, noise, timesteps)

        # Concatenate the low-resolution and noisy high-resolution images
        x_t = torch.cat((noisy_images, lr), dim=1)

        # Get the model's prediction (predicted noise)
        noise_pred = model(x_t, timesteps).sample

        # Predict the high-resolution image (denoised output)
        pred_hr = []
        for i in range(lr.shape[0]):  # Iterate over the batch
            scheduler_output = scheduler.step(
                model_output=noise_pred[i:i+1],  # Predicted noise for the current sample
                timestep=timesteps[i],           # Timestep for the current sample
                sample=noisy_images[i:i+1]       # Noisy image for the current sample
            )
            pred_hr.append(scheduler_output.prev_sample)

        # Stack predictions to form the batch
        pred_hr = torch.cat(pred_hr, dim=0)

        # Compute the noise prediction loss
        loss = criterion(noise_pred, noise)

        # Additional loss between predicted hr and z (zr)
        loss_z = masked_mse_loss(pred_hr, zr)

        # Combine the losses
        total_loss = loss + loss_z
        train_loss += loss.item()
        train_loss_z += loss_z.item()

        # Backpropagation and optimization
        optimizer.zero_grad()
        #total_loss.backward()
        loss_z.backward()
        optimizer.step()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    train_loss /= len(train_dataloader)
    train_loss_z /= len(train_dataloader)

    # Validation Loop
    model.eval()
    val_loss = 0.0
    val_loss_z = 0.0  # For additional loss

    with torch.no_grad():
        for batch in val_dataloader:
            lr, hr, zr = batch
            lr, hr, zr = lr.to(device), hr.to(device), zr.to(device)

            # Sample noise to add to the high-resolution images
            noise = torch.randn_like(hr)

            # Sample a random timestep for each image
            timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (lr.shape[0],)).long().to(device)

            # Get the noisy high-resolution images
            noisy_images = scheduler.add_noise(hr, noise, timesteps)

            # Concatenate the low-resolution and noisy high-resolution images
            x_t = torch.cat((noisy_images, lr), dim=1)

            # Get the model's prediction (predicted noise)
            noise_pred = model(x_t, timesteps).sample

            # Predict the high-resolution image (denoised output)
            pred_hr = []
            for i in range(lr.shape[0]):  # Iterate over the batch
                scheduler_output = scheduler.step(
                    model_output=noise_pred[i:i+1],  # Predicted noise for the current sample
                    timestep=timesteps[i],           # Timestep for the current sample
                    sample=noisy_images[i:i+1]       # Noisy image for the current sample
                )
                pred_hr.append(scheduler_output.prev_sample)

            # Stack predictions to form the batch
            pred_hr = torch.cat(pred_hr, dim=0)

            # Compute the noise prediction loss
            loss = criterion(noise_pred, noise)

            # Additional loss between predicted hr and z (zr)
            loss_z = masked_mse_loss(pred_hr, zr)

            # Combine the losses
            val_loss += loss.item()
            val_loss_z += loss_z.item()

    val_loss /= len(val_dataloader)
    val_loss_z /= len(val_dataloader)

    return train_loss, val_loss, train_loss_z, val_loss_z


# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Model setup
patch_size = 32
model = UNet2DModel(
    sample_size=patch_size,  # Input image size
    in_channels=2,           # Input channels (low resolution + noisy high resolution)
    out_channels=1,          # Output channels (high resolution)
    layers_per_block=4,
    block_out_channels=(64, 128, 256, 512),
).to(device)

# Scheduler setup
scheduler = DDPMScheduler(
    num_train_timesteps=1000,  # Number of diffusion steps
    beta_schedule="linear"
)

# Optimizer
optimizer = AdamW(model.parameters(), lr=1e-4)

# Loss function
criterion = nn.MSELoss()

# Hyperparameters
num_epochs = 1000
print_interval = 10
patience = 50
best_val_loss = float('inf')
counter = 0
best_model = None

# Dataloaders (assuming x_train_patches, y_train_patches, z_train_patches, etc., are defined)
train_dataset = ncDataset(x_train_patches, y_train_patches, z_train_patches)
val_dataset = ncDataset(x_val_patches, y_val_patches, z_val_patches)

train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=20, shuffle=True)

# Main Training Loop
for epoch in range(1, num_epochs + 1):
    train_loss, val_loss, train_loss_z, val_loss_z = train(
        model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, device
    )

    # Log the losses
    print(f"Epoch [{epoch}/{num_epochs}]")
    print(f"  Train Loss: {train_loss:.4f}, Train Loss Z: {train_loss_z:.4f}")
    print(f"  Val Loss: {val_loss:.4f}, Val Loss Z: {val_loss_z:.4f}")

    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model = deepcopy(model)
        counter = 0
    else:
        counter += 1

    if counter >= patience:
        print("Early stopping triggered.")
        break

print("Training Complete.")

